{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "prompt_resume = f\"\"\"\n",
        "  <|im_start|>system\n",
        "  Описание роли ЛЛМ\n",
        "  и задачи, которую мы ставим\n",
        "  one-shot и few-shot сюда же\n",
        "  <|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Вопрос к ЛЛМ\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "# все служебные команды модель красиво форматирует"
      ],
      "metadata": {
        "id": "TyYJZQrfOjHw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npPZi3OL3m02"
      },
      "source": [
        "Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w82jVRVvRlaE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"JeswinMS4/code_text_classification\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OwtzdjiJRri6",
        "outputId": "636dd55e-22f5-492a-ce3d-82a64b2a0d8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'Label', '__index_level_0__'],\n",
              "        num_rows: 823\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'Label', '__index_level_0__'],\n",
              "        num_rows: 46\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'Label', '__index_level_0__'],\n",
              "        num_rows: 46\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "ds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0EgjbEM32yU"
      },
      "source": [
        "Загрузка модели"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNKVa_oSAuxm"
      },
      "outputs": [],
      "source": [
        "  pip install bitsandbytes -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DNGXmjU3E6Ie"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-3B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",\n",
        "    load_in_4bit=True,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model successfully loaded on device:\", model.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNQTjmoP9put"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else cpu)\n",
        "model.to(device);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Подготовка данных"
      ],
      "metadata": {
        "id": "_Qj6tClTy2IE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xg3tgl8PRrl0",
        "outputId": "606a26ad-63a2-4401-dd27-e6acbac37596"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (46, 4)\n"
          ]
        }
      ],
      "source": [
        "def data_preparation(train_size, test_size):\n",
        "  sample_train = ds[\"train\"].select(range(train_size))\n",
        "  sample_df_train = sample_train.to_pandas()\n",
        "  sample_df_train['bin'] = sample_df_train['Label'].apply(lambda x: 1 if x=='Code Generation' else 0)\n",
        "\n",
        "  sample_test = ds[\"test\"].select(range(test_size))\n",
        "  sample_df_test = sample_test.to_pandas()\n",
        "  sample_df_test['bin'] = sample_df_test['Label'].apply(lambda x: 1 if x=='Code Generation' else 0)\n",
        "  return sample_df_train, sample_df_test\n",
        "sample_df_train, sample_df_test = data_preparation(100, 46)\n",
        "print(sample_df_train.shape, sample_df_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(sample_df_test.sentence.unique())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TuLLb0vHESQy",
        "outputId": "9a054a83-a407-46e1-8672-0f2b61991f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "46"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RkUF9unjsMco",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "051778b5-62e9-4a10-e718-53f922ba2b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(96, 4)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# FewShotInstructions\n",
        "print(sample_df_train.shape)\n",
        "few_shot_instructions = []\n",
        "drop_ids = []\n",
        "for lbl in range(2):\n",
        "  for num in range(2):\n",
        "    few_shot_instruction = ' '.join([\n",
        "                                      \"<|im_start|>user\\n \",\n",
        "                                      \"Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\\n\",\n",
        "                                      f\"{sample_df_train[sample_df_train.bin==lbl]['sentence'].iloc[num]} \\n\",\n",
        "                                      \"<|im_end|>\\n\",\n",
        "                                      \"<|im_start|>assistant \\n\",\n",
        "                                      str(lbl),\n",
        "                                      \"\\n\",\n",
        "                                      ])\n",
        "    drop_ids.append(sample_df_train[sample_df_train.bin==lbl].iloc[num]['__index_level_0__'])\n",
        "    few_shot_instructions.append(few_shot_instruction)\n",
        "few_shot_instructions = '\\n'.join(few_shot_instructions)\n",
        "sample_df_train = sample_df_train[~sample_df_train['__index_level_0__'].isin(drop_ids)]\n",
        "sample_df_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пример вывода модели"
      ],
      "metadata": {
        "id": "o0-2-6TYUMw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = sample_df_train['sentence'].iloc[0]\n",
        "prompt = f\"\"\"\n",
        "\n",
        "  <|im_start|>system\n",
        "  You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "  Ты — классификатор текстов.\n",
        "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
        "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.<|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
        "  {text}\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.9)\n",
        "result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vguRMHytUL_G",
        "outputId": "25694c38-f726-4b95-c7bc-af531b027cd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "  system\n",
            "  You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
            "  Ты — классификатор текстов.\n",
            "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
            "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.\n",
            "\n",
            "  user\n",
            "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
            "  What is the chemical formula for carbon monoxide?\n",
            "  \n",
            "  assistant\n",
            "  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Строим модель"
      ],
      "metadata": {
        "id": "ruhNTVGZAwkh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "0PR61bGI45LL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Prompt Test  {few_shot_instructions}\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for cnt in (range(sample_df_train.shape[0])):\n",
        "  text = sample_df_train['sentence'].iloc[cnt]\n",
        "  prompt = f\"\"\"\n",
        "\n",
        "  <|im_start|>system\n",
        "  You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\n",
        "  Ты — классификатор текстов.\n",
        "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
        "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.<|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
        "  {text}\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.9)\n",
        "  result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  # print(result)\n",
        "  if \"0\" not in result and \"1\" not in result:\n",
        "    print(\"Модель ответила неожиданно.\")\n",
        "    result = None\n",
        "\n",
        "  # print(result)\n",
        "  # print('raw', result.split('<im_start>assistant\\n')[-1].split())\n",
        "  # print('cut', result.split('<im_start>assistant\\n')[-1].split()[0])\n",
        "\n",
        "  try:\n",
        "    # print('pred:', int(resu split('<im_start>assistant\\n')[-1].split()[0]), 'true:', sample_df_test['bin'].iloc[cnt])\n",
        "    y_pred.append(int(result.split('assistant\\n')[-1].split()[0]))\n",
        "    y_true.append(int(sample_df_train['bin'].iloc[cnt]))\n",
        "  except:\n",
        "    # print(result.split('assistant\\n')[-1].split()[0])\n",
        "    print('\\n \\n smth wrong', cnt)\n",
        "  # print('---'*20)\n",
        "\n",
        "sum_d2 = []\n",
        "for i in range(len(y_pred)):\n",
        "  sum_d2.append((y_pred[i]+y_true[i])/2)\n",
        "from collections import Counter\n",
        "cntr = Counter(sum_d2)\n",
        "Accuracy = (cntr[0.0]+cntr[1.0])/len(sum_d2)\n",
        "Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_-jLD9fSds8Q",
        "outputId": "8d5a55c5-5864-4d14-c7e0-fc6dce2a91d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5625"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AX-QXtOAFCXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Prompt Test  {few_shot_instructions}\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for cnt in (range(sample_df_train.shape[0])):\n",
        "  text = sample_df_train['sentence'].iloc[cnt]\n",
        "  prompt = f\"\"\"\n",
        "  <|im_start|>system\n",
        "  Ты — классификатор текстов.\n",
        "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
        "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.<|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
        "  {text}\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.9)\n",
        "  result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  # print(result)\n",
        "  if \"0\" not in result and \"1\" not in result:\n",
        "    print(\"Модель ответила неожиданно.\")\n",
        "    result = None\n",
        "\n",
        "  # print(result)\n",
        "  # print('raw', result.split('<im_start>assistant\\n')[-1].split())\n",
        "  # print('cut', result.split('<im_start>assistant\\n')[-1].split()[0])\n",
        "\n",
        "  try:\n",
        "    # print('pred:', int(resu split('<im_start>assistant\\n')[-1].split()[0]), 'true:', sample_df_test['bin'].iloc[cnt])\n",
        "    y_pred.append(int(result.split('assistant\\n')[-1].split()[0]))\n",
        "    y_true.append(int(sample_df_train['bin'].iloc[cnt]))\n",
        "    print(len(y_pred), len(y_true), end='---')\n",
        "  except:\n",
        "    print('smth wrong', cnt)\n",
        "  # print('---'*20)\n",
        "\n",
        "sum_d2 = []\n",
        "for i in range(len(y_pred)):\n",
        "  sum_d2.append((y_pred[i]+y_true[i])/2)\n",
        "from collections import Counter\n",
        "cntr = Counter(sum_d2)\n",
        "Accuracy = (cntr[0.0]+cntr[1.0])/len(sum_d2)\n",
        "Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ijduKz9d5up",
        "outputId": "091b8d29-053a-4396-b928-4ec148ceb5ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1---2 2---3 3---4 4---5 5---6 6---7 7---8 8---9 9---10 10---smth wrong 10\n",
            "11 11---12 12---13 13---14 14---15 15---16 16---17 17---18 18---19 19---20 20---21 21---22 22---23 23---24 24---25 25---26 26---27 27---28 28---29 29---30 30---31 31---32 32---33 33---34 34---35 35---36 36---37 37---38 38---39 39---40 40---41 41---42 42---43 43---44 44---45 45---46 46---47 47---48 48---49 49---50 50---51 51---52 52---53 53---54 54---55 55---56 56---57 57---58 58---59 59---60 60---61 61---62 62---63 63---64 64---65 65---66 66---67 67---68 68---69 69---70 70---71 71---72 72---73 73---74 74---75 75---76 76---77 77---78 78---79 79---80 80---81 81---82 82---83 83---84 84---85 85---86 86---87 87---88 88---89 89---90 90---91 91---92 92---93 93---94 94---95 95---"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6526315789473685"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Prompt Test  {few_shot_instructions}\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for cnt in (range(sample_df_train.shape[0])):\n",
        "  text = sample_df_train['sentence'].iloc[cnt]\n",
        "  prompt = f\"\"\"\n",
        "  <|im_start|>system\n",
        "  Ты — классификатор текстов.\n",
        "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
        "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.\n",
        "\n",
        "  Примеры правильных ответов:\n",
        "  {few_shot_instructions}\n",
        "  Конец примеров.\n",
        "  <|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
        "  {text}\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.9)\n",
        "  result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  # print(result)\n",
        "  if \"0\" not in result and \"1\" not in result:\n",
        "    print(\"Модель ответила неожиданно.\")\n",
        "    result = None\n",
        "\n",
        "  # print(result)\n",
        "  # print('raw', result.split('<im_start>assistant\\n')[-1].split())\n",
        "  # print('cut', result.split('<im_start>assistant\\n')[-1].split()[0])\n",
        "\n",
        "  try:\n",
        "    # print('pred:', int(resu split('<im_start>assistant\\n')[-1].split()[0]), 'true:', sample_df_test['bin'].iloc[cnt])\n",
        "    y_pred.append(int(result.split('assistant\\n')[-1].split()[0]))\n",
        "    y_true.append(int(sample_df_train['bin'].iloc[cnt]))\n",
        "    print(len(y_pred), len(y_true), end='---')\n",
        "  except:\n",
        "    print('smth wrong', cnt)\n",
        "  # print('---'*20)\n",
        "\n",
        "sum_d2 = []\n",
        "for i in range(len(y_pred)):\n",
        "  sum_d2.append((y_pred[i]+y_true[i])/2)\n",
        "from collections import Counter\n",
        "cntr = Counter(sum_d2)\n",
        "Accuracy = (cntr[0.0]+cntr[1.0])/len(sum_d2)\n",
        "Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWqOUbY36Enf",
        "outputId": "e99b1f20-2e5e-458c-804e-f77d0da7e7dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1---2 2---3 3---4 4---5 5---6 6---7 7---8 8---9 9---10 10---11 11---12 12---13 13---14 14---15 15---16 16---17 17---18 18---19 19---20 20---21 21---22 22---23 23---24 24---25 25---26 26---27 27---28 28---29 29---30 30---31 31---32 32---33 33---34 34---35 35---36 36---37 37---38 38---39 39---40 40---41 41---42 42---43 43---44 44---45 45---46 46---47 47---48 48---49 49---50 50---51 51---52 52---53 53---54 54---55 55---56 56---57 57---58 58---59 59---60 60---61 61---62 62---63 63---64 64---65 65---66 66---67 67---68 68---69 69---70 70---71 71---72 72---73 73---74 74---75 75---76 76---77 77---78 78---79 79---80 80---81 81---82 82---83 83---84 84---85 85---86 86---87 87---88 88---89 89---90 90---91 91---92 92---93 93---94 94---95 95---96 96---"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.84375"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Inner Prompt Test  {few_shot_instructions}\n",
        "y_pred = []\n",
        "y_true = []\n",
        "for cnt in (range(sample_df_test.shape[0])):\n",
        "  text = sample_df_test['sentence'].iloc[cnt]\n",
        "  prompt = f\"\"\"\n",
        "  <|im_start|>system\n",
        "  Ты — классификатор текстов.\n",
        "  Задача: Классификация запросов: запрос на генерацию кода (1), либо генерацию текста (0).\n",
        "  Отвечай ТОЛЬКО \"1\" или \"0\", без объяснений.\n",
        "\n",
        "  Примеры правильных ответов:\n",
        "  {few_shot_instructions}\n",
        "  Конец примеров.\n",
        "  <|im_end|>\n",
        "\n",
        "  <|im_start|>user\n",
        "  Это запрос на генерацию кода (1) или текста (0)? Ответь ТОЛЬКО 1 или 0.\n",
        "  {text}\n",
        "  <|im_end|>\n",
        "  <|im_start|>assistant\n",
        "  \"\"\"\n",
        "  inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "  outputs = model.generate(**inputs, max_new_tokens=5, temperature=0.9)\n",
        "  result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  # print(result)\n",
        "  if \"0\" not in result and \"1\" not in result:\n",
        "    print(\"Модель ответила неожиданно.\")\n",
        "    result = None\n",
        "\n",
        "  # print(result)\n",
        "  # print('raw', result.split('<im_start>assistant\\n')[-1].split())\n",
        "  # print('cut', result.split('<im_start>assistant\\n')[-1].split()[0])\n",
        "\n",
        "  try:\n",
        "    # print('pred:', int(resu split('<im_start>assistant\\n')[-1].split()[0]), 'true:', sample_df_test['bin'].iloc[cnt])\n",
        "    y_pred.append(int(result.split('assistant\\n')[-1].split()[0]))\n",
        "    y_true.append(int(sample_df_test['bin'].iloc[cnt]))\n",
        "    print(len(y_pred), len(y_true), end='---')\n",
        "  except:\n",
        "    print('smth wrong', cnt)\n",
        "  # print('---'*20)\n",
        "\n",
        "sum_d2 = []\n",
        "for i in range(len(y_pred)):\n",
        "  sum_d2.append((y_pred[i]+y_true[i])/2)\n",
        "from collections import Counter\n",
        "cntr = Counter(sum_d2)\n",
        "Accuracy = (cntr[0.0]+cntr[1.0])/len(sum_d2)\n",
        "Accuracy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WDZ5-wYAdjS3",
        "outputId": "2fb2cfa9-5caf-4514-d4e3-69939b61a2a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1 1---2 2---3 3---4 4---5 5---6 6---7 7---8 8---9 9---10 10---11 11---12 12---13 13---14 14---15 15---16 16---17 17---18 18---19 19---20 20---21 21---22 22---23 23---24 24---25 25---26 26---27 27---28 28---29 29---30 30---31 31---32 32---33 33---34 34---35 35---36 36---37 37---38 38---39 39---40 40---41 41---42 42---43 43---44 44---45 45---46 46---"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8478260869565217"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YcrKzu6mHzAu"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UmIA5uFrHzEZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}